\section{Datasets} \label{sec:datasets}

When researching \gls{OGM} prediction using deep learning networks, it is important to choose a dataset (or multiple datasets) that optimally fulfills the requirements for that purpose. The dataset will be used to train and evaluate the deep learning network on. Not only the performance of deep learning network, but also the time that is required to pre-process, train, and evaluate the network is greatly dependent on what dataset is used. Also, the performance of a network should be compared reliably. Therefore, it is important that a dataset is chosen on which other research has based their network training and evaluation on, so there are enough results for reliable comparisons. To ensure that the optimal dataset is chosen for \gls{OGM} prediction research, the following list of criteria is devised which the dataset has to meet. Based on how the dataset scores on this list, an informed decision can be made. 


% Do these criteria have to contain concrete numerical bounds, or is this good? If so, to these concrete numerical bounds have to be based on sources, or can I base them on my own insights?
\begin{enumerate}
	\item The dataset contains data of ego-vehicle centered traffic scene sequences that provide at least 2D \gls{BEV} information of the environment.
	\item The dataset provides enough diverse data to train and evaluate a network on.
	\item The dataset contains traffic scene sequences, or provides means to easily generate them.
	\item The sequences have enough frames per sequence and a frame frequency that is suitable for capturing road user behavior and trajectories.
	\item The sequences contain various traffic actors including \glspl{VRU}.
	\item The dataset data, and any \gls{OGM} that can be generated from it, provides a resolution that is high enough to distinguish and to track \glspl{VRU}. 	
	\item The sequences show a diversity in environmental properties, containing \glspl{VRU}, that may influence the generated \glspl{OGM} (e.g. urban vs rural, dense vs sparse traffic).
	\item If any extended \gls{OGM} form is required for research, the dataset should also provide means to attain data that is required for such an extended form including ground truth data.
	\item Results of other research using the dataset for generating and predicting \glspl{OGM} is available for comparison. 
\end{enumerate}


% Observation: The only OGM prediction dataset is KITTI, which also does not provide any OGMs and is now relatively small compared to state-of-the-art datasets. There is a lack of good datasets that provide easy means to create OGMs from their data. Maybe thesis topic: Create OGMs from other, bigger, datasets to perform OGM prediction on. Methods that predict OGMs (given OGMs) either use their own dataset which is small, or they use KITTI. Also, Nuscenes is used, but the input is a multi-layer BEV map which does not contain uncertainty information. Then, one paper uses the Waymo Open Dataset which is a large dataset \cite{toyungyernsub2020double}. This paper also generates OGMs from LiDAR data, so the Waymo dataset does not provide a standardized means to generate the OGM self. It is however the most useful dataset for OGM prediction. Also, the OGMs that \cite{toyungyernsub2020double} describes could be extended with segmentation of traffic signs/zebra crossings/other motion cues for VRUs. 

There is a number of existing datasets that are closest to meeting all criteria. Three datasets were obtained for a purpose that includes motion prediction. Five datasets were obtained for object detection purposes. The last two suitable datasets were mainly obtained to research semantic segmentation on. All the datasets are recorded using a vehicle equipped with one or multiple sensors that drives through real traffic. The datasets are discussed below in three subsections based on their original purposes: Motion prediction, Object Detection, and Semantic Segmentation. At the end of this chapter is a conclusion about the datasets.


\subsection{Tracking and Motion prediction Datasets}
The following three datasets are obtained for tracking or motion prediction purposes. The \gls{STIP} dataset \cite{liu2020spatiotemporal}, the Argoverse dataset \cite{chang2019argoverse} and the RoboCar dataset \cite{robotcardatasetijrr}. \\

The \gls{STIP} dataset \cite{liu2020spatiotemporal} is obtained using three cameras (left, front and right) with a 1216x1936 resolution at 20 Hz positioned on the recording vehicle. The data is recorded in dense urban areas in 8 US cities (in California and Michigan). The dataset contains 923.48 minutes of driving scenes, which comes down to 1,108,176 frames. The data contains 350K pedestrian instances annotated with 2D bounding boxes at 2 fps which are interpolated to cover all frames with pedestrians. There are over 25K pedestrian tracks with a median length of 4 seconds, at 20 Hz. 556 sequences are selected in which busy intersections are recorded. These sequences are subdivided into a training set containing 2525 pedestrians in 102.37 minutes of video, and a test set containing 823 pedestrians in 23.43 minutes of video. \\

For the Argoverse dataset \cite{chang2019argoverse} two 107K points LiDARs, with a range of 200m, at 10 Hz together with seven 1920x1200 resolution cameras in a 360 degrees view setting at 30 Hz, with two additional 2056x2464 resolution front cameras at 5 Hz were used to obtain the data. Localization data is obtained from GPS and motion sensors. The data is recorded in 2 US cities (Pittsburgh and Miami). The dataset contains over 320K five-second sequences containing the centroid of each tracked object in 2D \gls{BEV} sampled at 10 Hz. Together these are over 19K minutes containing over 11K tracked road users and obstacles labeled out of 17 categories including vehicles, pedestrians and cyclists. \\

The RoboCar dataset \cite{robotcardatasetijrr} data is collected using three LiDARs with each a range of 50m at 50 Hz, and four cameras (one 1280x960 resolution camera in front at 16 Hz, and three 1024x1024 resolution cameras at 11.1 Hz at the rear and on the sides). A GPS INS combination is used for localization. The data is recorded in Oxford, UK. A 1000 km of traffic scenes are recorded and subdivided into 360s second sequences. Vehicles, pedestrians and other road users are recorded in the dataset, however none are labelled. It is therefore unkown how many pedestrians or vehicles are recorded.


\subsection{Object Detection Datasets}
This subsection will discuss five datasets that were originally obtained for object detection purposes. The \gls{ECP2.5D} dataset \cite{braun2020ecp2}, the \gls{BDD100K} dataset \cite{yu2020bdd100k}, the \gls{KITTI} dataset \cite{geiger2012we}, the nuScenes dataset \cite{caesar2020nuscenes}, and the Waymo Open dataset \cite{sun2020scalability}. \\

The \gls{ECP2.5D} dataset \cite{braun2020ecp2} is obtained using a Velodyne HDL-64E LiDAR, which has a range of 120m and a rotation frequency of 5-20 Hz, and a 1920x1014 resolution front camera at 20 Hz. The localization data is obtained using a GPS INS combination. The recordings were made in 30 different cities in 12 diverse European countries. The dataset contains 136K persons in 46K frames of 15s sequences at 20 Hz. Together there are 218K pedestrians and 19K riders. More than 140K 2D person annotations are made, of which around 136K have 3D information. \\

The \gls{BDD100K} dataset \cite{yu2020bdd100k} is collected using crowd-sourcing. Drivers could upload their data obtained by using a 1280x720 resolution front camera at 30 Hz together with GPS/IMU localization data. Recordings were mostly made in San Francisco and the Bay Area, New York, and Berkeley. The dataset contains data for object detection, but also for tracking and semantic segmentation, since object detection is not the sole purpose of the \gls{BDD100K} dataset. The dataset contains 100K driving videos, each of a duration of 40 seconds. For all 100K videos, 10 object categories are annotated with 2D bounding boxes and 8 lane marking categories and the vehicle's drivable area are annotated on a pixel-level. Other annotations for object detection and semantic segmentation are done for every 10th second in each video. For those 10K sample frames 40 object classes are annotated on a pixel-level. For the tracking task, 2K 40-second videos containing about 400K frames are annotated at 4 Hz. The dataset contains 130.6K track identities and 3.3M bounding boxes for the training and validation set. There are 129K instances of pedestrians and about 1M vehicles in the dataset. \\

The \gls{KITTI} dataset \cite{geiger2012we} is obtained using a Velodyne HDL-64E LiDAR with a 120m range at 10 Hz and four 1392x512 resolution cameras (two color cameras and two grayscale cameras) at 10 Hz. A GPS/IMU combination is used for localization data. The data is recorded in one city (Karlsruhe, Germany). Besides object detection, this dataset also provides data for stereo vision, optical flow, visual odometry, SLAM, and 3D object tracking tasks. Dynamic objects of 8 different classes, including vehicles and pedestrians, are annotated in the form of 3D bounding box tracklets in the LiDAR pointclouds, which are projected into the camera images. 9400 pedestrians are annotated and 3300 riders. The dataset contains 22 sequences in traffic that together span a distance of 39.2 km and consist of 41K frames at 10 Hz. \\

% KITTI: \cite{geiger2012we} or  \cite{geiger2013vision} ? 
For the nuScenes dataset \cite{caesar2020nuscenes}, a LiDAR that can generate a 1.4M point 360 degree view at 20 Hz, five radars around the vehicle with a 250m range at 13 Hz, and six 1600x900 resolution cameras around the vehicle at 12 Hz are used to collect data. Also a GPS/IMU and CAN bus data is collected for localization. The dataset is recorded in 2 cities (Boston and Singapore). There is 15 hours of data that together span a total distance of 242 km of diverse driving conditions. The nuScenes dataset is collected for object detection as well as object tracking. Therefore, it contains 1000 sequences of 20 seconds, annotated at 2 Hz. Objects of 23 classes are annotated, including vehicles, pedestrians, and riders. The nuScenes paper \cite{caesar2020nuscenes} compares its dataset against the \gls{KITTI} dataset \cite{geiger2012we}. It concludes that training on the \gls{KITTI} dataset, with its smaller size compared to nuScenes, affects a network's performance. \\

% https://github.com/sisl/Double-Prong-Occupancy waymo to OGM
The Waymo Open dataset \cite{sun2020scalability} (in short Waymo), is obtained using five LiDAR sensors and five cameras (three 1920x1280 resolution front cameras and two 1920x1040 side cameras) sampled at 10 Hz. The dataset was originally recorded in 3 cities (Phoenix, San Francisco, and Mountain View) and later extended with another 3 cities (Los Angeles, Detroit, and Seattle). Besides object detection, the Waymo dataset's extension focuses on data for motion prediction. It now contains 103K 20 second sequences at 10 Hz (together over 20M frames and 574 hours of data). It contains 10.8M objects with tracking IDs, labels for three object classes (vehicles, pedestrians, and cyclist), and 3D bounding boxes of each of those objects. Each sequences contains 3D map data and is further broken down into 9 second windows (1 second of historic frames and 8 seconds of future frames) with 5 second overlap for motion prediction purposes.  


\subsection{Semantic Segmentation Datasets}
This section discusses two datasets that are originally generated for semantic segementation tasks. The Apolloscape dataset \cite{huang2019apolloscape}  and the Cityscapes dataset \cite{cordts2016cityscapes}. \\

Apolloscape \cite{huang2019apolloscape} is a dataset which is collected for the purpose of scene parsing (semantic segmentation on pixel-level). The data is obtained using two VUX-1HA LiDARs and a VMX0CS6 camera system with two 3384x2710 resolution cameras. Localization data is collected with a IMU/GNSS combination. The dataset contains almost 144K frames with pixel-level annotations for semantic segmentation. Furthermore, 89K instance-level annotations are provided for movable objects. 25 different labels are annotated covering five groups. Also, 28 lane markings are annotated. Together there are 543K pedestrian instances and 1.99M vehicle instances in the dataset. \\

The Cityscapes dataset \cite{cordts2016cityscapes} is obtained using a 1920x1080 resolution stereo camera (OnSemi AR0331) at 17 Hz. 16 bit HDR and 8 bit LDR RGB images are recorded. Localization data is collected with a GPS/odometry sensor combination and the outside temperature is measured as well. The dataset is recorded in 50 different cities, primarily in Germany but also in neighboring countries. From 27 cities, 5000 images are selected for dense pixel-level annotation. Annotations are done on every 20th frame of a 30-frame video sequence. For the remaining 23 cities, coarse annotation is performed on a single image every 20 seconds or 20 meters driving distance (whichever comes first). This yields another 20K annotated images. 30 classes are annotated which are grouped into 8 categories. The dataset contains 24.4K annotated pedestrians and 41K vehicles. Humans and vehicles are also annotated on an instance level. 

\subsection{What dataset is most suitable to generate \gls{OGM} sequences for \gls{OGM} prediction?}

Table \ref{tab:datasets_overview} shows an overview of the datasets that are discussed in the previous subsections. Based on the properties of each dataset it is evaluated how well they meet the criteria that were set at the beginning of this chapter. If a criterion is met, the score of the dataset goes up by 1 point. If the criterion is met partially, or in average quality compared to the other datasets, the score goes up by 0.5 point. If a criterion is not met, or if it is met in a bad quality compared to other datasets, the dataset will not get any points for that criterion. Also, if information about a criterion is not found or not available, no points are given to the criterion. Table \ref{tab:datasets_criteria} shows an overview of how well each datasets scores per criterion. More information about the symbols and what score is linked to them is written below the table. \\

With a score of 7.5, the Waymo \cite{sun2020scalability} dataset is the most suitable dataset to generate \gls{OGM} sequences for \gls{OGM} prediction. While the diversity of the Waymo dataset is average, compared to the other datasets, and there is no (available) data to extend the \gls{OGM} form, the Waymo dataset meets all other criteria in good quality which sums up to the score of 7.5. Other suitable datasets on the second and shared third places are the Cityscapes \cite{cordts2016cityscapes}, Argoverse \cite{chang2019argoverse} and NuScenes \cite{caesar2020nuscenes} datasets with a score of 7.0, 6.5 and 6.5 respectively. Unlike the Waymo dataset, the Cityscapes dataset provides pixel-level semantic segmentation data and the Nuscenes dataset provides velocity data. In the case this additional data is desired for research on a specific \gls{OGM} prediction method, the Cityscapes or Nuscenes datasets can be more suitable to use than the Waymo dataset. Also, conveniently, for the four best scoring datasets there is literature available that provides code to generate \glspl{OGM} from the data. These sources can be found in table \ref{tab:datasets_overview}.  

\begin{table}[h!]
	\centering
	\resizebox{\linewidth}{!}{%
		\begin{tabular}{lllllllllll} 
			\toprule
			& \textbf{STIP \cite{liu2020spatiotemporal}} & \textbf{Argoverse \cite{chang2019argoverse}} & \textbf{ECP2.5D \cite{braun2020ecp2}} & \textbf{BDDK100 \cite{yu2020bdd100k}} & \textbf{KITTI \cite{geiger2012we}} & \textbf{NuScenes \cite{caesar2020nuscenes}} & \textbf{Waymo \cite{sun2020scalability}} & \textbf{Apolloscape \cite{huang2019apolloscape}} & \textbf{Cityscapes \cite{cordts2016cityscapes}} & \textbf{RoboCar \cite{robotcardatasetijrr}} \\ 
			\hline
			\textbf{1} & N & Y & Y & N & Y & Y & Y & Y & Y & Y \\
			\textbf{2} & A & A & A & G & B & A & A & B & G & B \\
			\textbf{3} & A & G & A & G & B & G & G & U & U & U \\
			\textbf{4} & G & G & G & U & G & B & G & U & G & G \\
			\textbf{5} & Y & Y & Y & Y & Y & Y & Y & Y & Y & U \\
			\textbf{6} & Y & Y & Y & Y & Y & Y & Y & Y & Y & Y \\
			\textbf{7} & A & A & A & A & A & A & G & A & A & A \\
			\textbf{8} & - & - & - & Pixel Semantics & - & Velocity data & - & Pixel Semantics & Pixel Semantics & - \\
			\textbf{9} & - & Only OGM gen & - & - & OGM gen and pred & Only OGM gen & OGM gen and pred & - & Only OGM gen & OGM gen and pred \\
			\textbf{Scores} & 4.5 & \textbf{6.5} & 5.5 & 5.5 & 5.5 & \textbf{6.5} & \textbf{7.5} & 4.5 & \textbf{7.0} & 4.5 \\
			\bottomrule
		\end{tabular}
	}
	\caption{
		The table shows for each dataset how well it scores per criterion. The scores are based on the information in table \ref{tab:datasets_overview}. The following letters are used to evaluate each criterion: 
		\textbf{G:} Good, \textbf{A:} Average, \textbf{B:} Bad, \textbf{U:} Unknown: \textbf{-:} None/Not available/Not found, \textbf{Y:} Yes, \textbf{N:} No. \\\hspace{\textwidth} 
		At the bottom row, each dataset is given a score based on which letter is given at the evaluation, and whether there is useful information found for criteria 8 and 9. A 'G', a 'Y', 'OGM gen and pred', and any available additional data for an extended OGM form provide 1 point, an 'A' and 'only OGM gen' provides 0.5 points, and the 'N', 'B', 'U', and '-' provide 0 points.  
		Below is the list of criteria. \\\hspace{\textwidth} 	
		\textbf{1.} The dataset contains data of ego-vehicle centered traffic scene sequences that provide at least 2D BEV information of the environment.
		\textbf{2.} The dataset provides enough diverse data to train and evaluate a network on.
		\textbf{3.} The dataset contains traffic scene sequences, or provides means to easily generate them.
		\textbf{4.} The sequences have enough frames per sequence and a frame frequency that is suitable for capturing road user behavior and trajectories.
		\textbf{5.} The sequences contain various traffic actors including VRU.
		\textbf{6.} The dataset data, and any OGM that can be generated from it, provides a resolution that is high enough to distinguish and to track VRU. 
		\textbf{7.} The sequences show a diversity in environmental properties, containing VRU, that may influence the generated OGM (e.g. urban vs rural, dense vs sparse traffic).
		\textbf{8.} If any extended OGM form is required for research, the dataset should also provide means to attain data that is required for such an extended form including ground truth data.
		\textbf{9.} Results of other research using the dataset for generating and predicting OGM is available for comparison.}
	\label{tab:datasets_criteria}
\end{table}


\afterpage{%
	\clearpage% Flush earlier floats (otherwise order might not be correct)
	\thispagestyle{empty}% empty page style (?)
	\begin{landscape}
		\begin{longtable}{m{4em}m{2em}m{7em}m{7em}m{5em}m{2em}m{5em}m{7em}m{4em}m{2em}m{2em}m{5em}m{1em}m{1em}} 
			\toprule
			\rotatebox{90}{\textbf{Dataset}} & \rotatebox{90}{\textbf{Year}} & \multicolumn{3}{c}{\rotatebox{90}{\textbf{Sensors}}} & \rotatebox{90}{\textbf{Sampling Frequency}} & \rotatebox{90}{\textbf{Diversity of Locations}} & \rotatebox{90}{\textbf{Dataset size}} & \rotatebox{90}{\textbf{Diversity of labels}} & \rotatebox{90}{\textbf{\# Pedestrians}} & \rotatebox{90}{\textbf{\# Vehicles}} & \rotatebox{90}{\textbf{Extended OGMs}} & \rotatebox{90}{\textbf{OGM generation method}} & \rotatebox{90}{\textbf{OGM prediction comparison}} \endfirsthead 
			\midrule
			\textbf{STIP \cite{liu2020spatiotemporal}} & 2020 & - & 3 RGB cameras of res 1216x1936  (20 Hz) & - & 20 Hz & 1 country, 8 cities, dense urban areas & 556 sequences, \textasciitilde{}2h of recording, \textasciitilde{}150K frames & 2 classes (pedestrian, vehicle) & \textasciitilde{}3K & - & - & - & - \\
			\textbf{Argoverse \cite{chang2019argoverse}} & 2019 & 2 VLP-32 LiDARs with 200m range (10 Hz) & 7 RGB cameras in 360 deg setup of res 1920x1200 (30 Hz) and 2 in stereo view of res  2056x2464 (5 Hz) & GPS/ Odometry sensor & 10 Hz & 1 country, 2 cities, dense urban areas & \textasciitilde{}333K 5s sequences, 1006h of recording, spanning \textasciitilde{}290km & 17 classes & \textasciitilde{}1.5K & \textasciitilde{}8K & - & \cite{roddick2020predicting} & - \\
			\textbf{ECP2.5D \cite{braun2020ecp2}} & 2020 & Velodyne HDL-64E LiDAR with 120m range (5-20 Hz) & 1 RGB camera of res 1920x1024 (20 Hz) & GPS/INS & 20 Hz & 12 countries, 31 cities, dense urban areas & 15s sequences, 46K frames & 7 classes & \textasciitilde{}218K & - & - & - & - \\
			\textbf{BDDK100 \cite{yu2020bdd100k}} & 2020 & - & 1 camera of res 1280x720 (30 Hz) & GPS/IMU & - & (mostly) 1 country, 4 cities, dense urban areas & \textasciitilde{}100K 40s sequences & 3 categories, 20 classes & \textasciitilde{}129K & \textasciitilde{}1M & Pixel-level semantic segmentation & - & - \\
			\textbf{KITTI \cite{geiger2012we}} & 2012 & Velodyne HDL-64 LiDAR with 100m range (10 Hz) & 4 cameras (2 color 2 grayscale) of res 1392x512 (10 Hz) & GPS/IMU & 10 Hz & 1 country, 1 city, dense urban areas & 22 sequences & 2 categories, 8 classes & \textasciitilde{}9.4K & \textasciitilde{}100K & - & \cite{itkina2019dynamic} \cite{mohajerin2019multi} \cite{lange2020attention} & \cite{itkina2019dynamic} \cite{mohajerin2019multi} \cite{lange2020attention} \\
			\textbf{NuScenes \cite{caesar2020nuscenes}} & 2019 & 32 beam LiDAR 1.4M points/s (20 Hz) + 5 Radars with 250m range (13 Hz) & 6 cameras of res 1600x900 cameras (12 Hz) & GPS/IMU, CAN bus data & 2 Hz & 2 countries, 2 cities, dense urban areas & 1000 20s sequences & 23 classes, 8 attributes & \textasciitilde{}200K & \textasciitilde{}500K & velocity information from Radar with 0.1km/h accuracy & \cite{roddick2020predicting} \cite{loukkal2021driving} & - \\
			\textbf{Waymo \cite{sun2020scalability}} & 2020 & 5 LiDAR sensors (1 mid range, 4 short range) & 5 cameras (3 front of res 1920x1280 and 2 side of res 1920x1014) & - & 10 Hz & 1 country, 3 cities, urban and suburban areas & 103K 20s sequences & 3 classes & \textasciitilde{}2.8M & \textasciitilde{}6.1M & - & \cite{lange2020attention} \cite{toyungyernsub2020double} & \cite{lange2020attention} \cite{toyungyernsub2020double} \\
			\textbf{Apolloscape \cite{huang2019apolloscape}} & 2018 & 2 VUX-1HA LiDARs with 420m range & 1 VMX-CS6 camera system with 2 cameras of res 3384x2710 (no depth information) & IMU/GNSS & - & 1 country, dense urban areas & - & 5 categories, 35 classes, additional 28 kinds of lane markings & \textasciitilde{}543K & \textasciitilde{}1.99M & Pixel-level semantic segmentation & - & - \\
			\textbf{Cityscapes \cite{cordts2016cityscapes}} & 2020 & - & 1 stereo camera of res 1920x1080 (OnSemi AR0331) (17 Hz) & GPS, outside temperature, in-vehicle odometry sensors & 17 Hz & 1 country, 50 cities, dense urban areas & A 'large set' of sequences & 8 categories, 30 classes & \textasciitilde{}24.4K & \textasciitilde{}41K & Pixel-level semantic segmentation & \cite{hehn2021fast} & - \\
			\textbf{RoboCar \cite{robotcardatasetijrr}} & 2016 & 2 SICK LMS-151 2D LiDARs with 50m range (50 Hz) and 1 SICK LD-MRS 3D LiDAR with 50m range (12.5 Hz) & 4 cameras (1 front of res 1280x960 at 16 Hz, 3 for sides and rear of res 1024x1024 at 11.1 Hz) & GPS/INS & - & 1 country, 1 city, urban area & 360s sequences of unknown amount & No labels & - & - & - & \cite{dequaire2018deep} \cite{wang2020l2r} & \cite{dequaire2018deep} \\
			\bottomrule
		\caption{This table shows an overview of the datasets that are most likely to be suitable for \gls{OGM} prediction purposes.}
		\label{tab:datasets_overview}	
		\end{longtable}	
	\end{landscape}
}

%%



\newpage

\section{Metrics} \label{sec:metrics}

The quantitative evaluation of \gls{OGM} prediction methods depends highly on what metric is used to assess a method's performance. The metric determines which errors are considered and how much they weigh in the evaluation. For \gls{OGM} prediction, it is important that the predictions that ensure safe motion planning for the \gls{AV} are evaluated as 'better' than predictions that might cause accidents. Ensuring safe motion planning means that no (additional) dangerous situations are caused when the \gls{AV} executes a path that is planned in the \gls{OGM} predictions, compared to a path that is planned in the ground truth \gls{OGM}. This means that a prediction must deviate minimally from the ground truth. Also, \glspl{AV} have less time to correct their paths when there are large deviations or deviations close to the \gls{AV}. Large displacements and errors close to the \gls{AV} should therefore be considered worse than small displacements and errors further from the \gls{AV}. Moreover, if there are deletions or additions of actors in the predictions, the \gls{AV} might perform dangerous confronting or evasive maneuvers respectively. Therefore, deletions and additions should be penalized more than displacements. These demands result in the following list of criteria that a metric must meet in order to evaluate an \gls{OGM} method that ensures that safer methods are considered better.  

\begin{enumerate}
	\item The metric can evaluate the \gls{OGM} as a whole.
	\item The metric can evaluate the \gls{OGM} on local and on global scale.
	\item The metric negatively weighs small displacements less than big displacements. 
	\item The metric negatively weighs errors close to the \gls{AV} (often the center of the \gls{OGM}) more than errors further from the \gls{AV}.
	\item The metric negatively weighs additions and deletions more than displacements.
\end{enumerate}


\subsection{Occupancy Grid Map Metrics}
A selection of five metrics that are used to measure the quality of \glspl{OGM} compared to the ground truth is made based on papers that research \gls{OGM} prediction methods. These metrics are evaluated in this section based on the criteria described in the introduction of this chapter. 

\subsubsection{\glsfirst{MSE}} 
The \glsfirst{MSE} metric is used by \cite{itkina2019dynamic}, \cite{lange2020attention}, and \cite{toyungyernsub2020double} to compare the \gls{OGM} predictions with the ground truth. \cite{lange2020attention} and \cite{toyungyernsub2020double} also use the \glsfirst{IS} metric which is discussed later in this chapter. The \gls{MSE} metric formula is shown in equation \ref{eq:mse}, where $I$ is an $m \times n$ \gls{OGM}, $K$ its prediction, and $i$ and $j$ the \gls{OGM} coordinates.

\begin{equation} \label{eq:mse}
	MSE = \frac{1}{mn}\sum_{i=0}^{m-1}\sum_{j=0}^{n-1}[I(i,j)-K(i,j)]^2
\end{equation}

The equation evaluates the predicted \gls{OGM} as a whole and sums up the square of each cell's difference in value compared to the ground truth. So, the higher \gls{MSE}, the more its cell's values differ from the ground truth on a global scale. \cite{toyungyernsub2020double}'s research shows that the \gls{MSE} increases with the prediction time horizon of the \gls{OGM}, which is expected because of the accumulation of prediction errors with every time step. Because of its simplicity, the \gls{MSE} is used to assess not only \glspl{OGM} but images in general. However, there are implicit assumptions when using the \gls{MSE} \cite{wang2009mean}. The main assumptions are that the \gls{MSE} treats spatial positions of the grid cells, the sign of the error, and the relationship between the prediction and the ground truth independently, while it treats each cell with equal importance. Therefore, the \gls{MSE} cannot recognize relations within an \gls{OGM}, consider context, or differentiate local errors from global errors. The \gls{MSE} would thus not be able to assess displacements, deletions or additions. It is, however, possible to give errors close to the \gls{OGM}'s center a higher weighting by multiplying those errors by a penalty factor.    

\subsubsection{\glsfirst{SSIM}} 
The \gls{SSIM} is used by \cite{mohajerin2019multi} to evaluate the predicted \glspl{OGM} in their research. The \gls{SSIM} is based on the underlying principle "that the human visual system is highly adapted to extract structural information from visual scenes." \cite{wang2009mean}. This principle makes humans sensitive in identifying structural distortions in images, which is the reason that the \gls{SSIM} is designed to simulate that principle by focusing on image structure. Besides being sensitive for structural distortions, the human visual system also naturally compensates for non-structural distortions such as luminance (brightness) and contrast, since natural lighting causes a large variability in these distortions. The influence of luminance and contrast is separated from the structural distortions for better visual comprehension and recognition. The \gls{SSIM} task is therefore subdivided into three comparisons: luminance, contrast, and structure \cite{wang2004image}. These parts are explained as follows.\\

First the luminance of the \gls{OGM} is computed. The luminance is assumed to be the mean intensity of an \gls{OGM}. Equation \ref{eq:ssim_lumin} shows the formula for computing the luminance $\mu_X$, where $m \times n$ is the size of the \gls{OGM} and $X$ represents an \gls{OGM}. The luminance value of the ground truth and the prediction are compared using equation \ref{eq:ssim_lumin_compare}, where $X$ and $Y$ represent the predicted \gls{OGM} and ground truth respectively and $C_1 = (K_1 L)^2$, where $L$ is the dynamic range of the pixel intensity values (255 for 8-bit grids), and $K_1 \ll 1$ is a small constant.

\begin{equation} \label{eq:ssim_lumin}
	\mu_X = \frac{1}{mn}\sum_{i=0}^{m-1}\sum_{j=0}^{n-1}X_i(i,j)
\end{equation}
\begin{equation} \label{eq:ssim_lumin_compare}
	l(X,Y) = \frac{2\mu_X \mu_Y + C_1}{\mu_X ^2 + \mu_Y ^2 + C_1}
\end{equation}

Second, the \gls{OGM}'s contrast is computed. The contrast ratio is assumed to be the standard deviation $\sigma_X$ of the \gls{OGM}, shown in equation \ref{eq:ssim_contrast}. The contrasts of the prediction and the ground truth are then compared using equation \ref{eq:ssim_contrast_compare}, where $C_2 = (K_2 L)^2$, $L$ is the dynamic range of the pixel intensity values, and $K_2 \ll 1$ is a small constant.

\begin{equation} \label{eq:ssim_contrast}
	\sigma_X = \left(\frac{1}{mn-1}\sum_{i=0}^{m-1}\sum_{j=0}^{n-1}(X(i,j) - \mu_X)^2 \right)^\frac{1}{2}
\end{equation}
\begin{equation} \label{eq:ssim_contrast_compare}
	c(X,Y) = \frac{2\sigma_X \sigma_Y + C_2}{\sigma_X ^2 + \sigma_Y ^2 + C_2}
\end{equation}

Third, equation \ref{eq:ssim_structure_compare} is used to compare the structures between the prediction and the ground truth, where $C_3$ is a small constant and $\sigma_{XY}$ is computed using equation \ref{eq:ssim_structure_sig}.

\begin{equation} \label{eq:ssim_structure_compare}
	s(X,Y) = \frac{\sigma_{XY}+C_3}{\sigma_X\sigma_Y+C_3}
\end{equation}
\begin{equation} \label{eq:ssim_structure_sig}
	\sigma_{XY} = \frac{1}{mn-1}\sum_{i=0}^{m-1}\sum_{j=0}^{n-1}(X(i,j)-\mu_X)(Y(i,j) -\mu_Y)
\end{equation}

After the comparisons are performed, the \gls{SSIM} combines them according to equation \ref{eq:ssim}, "where $\alpha > 0$, $\beta > 0$ and $\gamma > 0$ are parameters used to adjust the relative importance of the three components." \cite{wang2004image}. For simplicity, these values can be set as $\alpha = \beta = \gamma = 1$ and $C_3 = C_2/2$ \cite{wang2004image}. The \gls{SSIM} ranges from $0$ to $1$, where $0$ means that the evaluated image is completely different from the ground truth, while a value of $1$ means that the evaluated image is identical to the ground truth.

\begin{equation} \label{eq:ssim}
	SSIM(x,y) = [l(x,y)]^\alpha \cdot [c(x,y)]^\beta \cdot [s(x,y)]^\gamma
\end{equation}

The \gls{SSIM} has the beneficial property that it can be used globally as well as locally (by separately evaluating spatial sections of an \gls{OGM}). Also, because the \gls{SSIM} is focused on structural similarities, it is expected that small displacements are weighed less than large displacement, and that displacements are weighed less than additions or deletions, since displacements retain the original structure, while the additions and deletions disrupt the original structure. Finally, similarly to the \gls{MSE}, the \gls{SSIM} can give a higher weighting to errors in the center of an \gls{OGM} if it is subdivided into sections and the center sections are given a higher error penalty.


\subsubsection{\glsfirst{IS}}
The \glsfirst{IS} metric is used in \cite{lange2020attention} and \cite{toyungyernsub2020double} to measure the predictions' quality of the scene structure compared to the ground truth. \cite{birk1996learning} developed the \gls{IS} metric which they later used in \cite{birk2006merging}'s research on finding similarities between \glspl{OGM} in order to merge them to generate bigger maps. \cite{birk1996learning} defines the \gls{IS} metrics according to  equations \ref{eq:is_met} and \ref{eq:is_met_dist}.


\begin{equation} \label{eq:is_met}
	IS(m_1, m_2) = \sum_{c \in C}^{}d(m_1, m_2,c) + d(m_2,m_1,c)
\end{equation}

\begin{equation} \label{eq:is_met_dist}
	d(m_1, m_2,c) = \frac{\sum_{m_1[p_1]=c}^{} min\{md(p_1, p_2)|m_2[p_2]=c\}}{\#_c(m_1)}
\end{equation}

Where $C$ denotes the set of 'colors', in the case of an \gls{OGM} empty, unknown and occupied. $m_i[p]$ is the color $c$ of grid $m_i$ at position $p=(x,y)$. $md(p_1,p_2) = |x_1 - x_2| + |y_1 - y_2|$ is the Manhattan-distance between $p_1$ and $p_2$. $\#_c(m_i) = \#{p_1|m_i[p_1] = c}$ is the number of pixels in $m_i$ with color $c$ \cite{birk1996learning}. \\
The \gls{IS} metric is the sum over all the grids colors (grid cell states) of the average Manhattan-distance of the cells with color $c$ (i.e. empty, unknown or occupied) in grid map $m_i$ to the nearest cell with color $c$ in grid map $m_j$ ($d(m_1, m_2,c)$) and the average Manhattan-distance vice versa \cite{birk1996learning}. The higher the \gls{IS} value, the larger the difference between the evaluated image and the ground truth. \\

The \gls{IS} metric, similarly to the \gls{SSIM}, compares image structures. Therefore, this metric can be used both on a global and local (spatial subsections) scale to compare \glspl{OGM}. Also, because \gls{IS} metric evaluates based on structure, it is expected that small displacements are weighed less than large displacements, and displacements in general are weighed less than deletions and additions. Like the other metrics, higher weights can be given to spatial subsections that match the \gls{AV}'s location (center of the grid map) to weigh errors close to the \gls{AV} more than errors farther away from the \gls{AV}.

\subsubsection{F1 score and \glsfirst{ROC}-curve}
To evaluate \gls{OGM} prediction quality, \cite{dequaire2018deep} uses the F1 score (equation \ref{eq:f1score}), \cite{hoermann2018dynamic} the \gls{ROC}-curve, and \cite{schreiber2019long} uses both. These two metrics together make use of the \gls{TPR} (Recall) in equation \ref{eq:recall}, \gls{PPV} (Precision) in equation \ref{eq:precision}, and \gls{FPR} (Fall-out) in equation \ref{eq:fallout} to determine how well the predictions match the ground truth. Here, a positive condition signifies that a grid cell is occupied, where a negative condition signifies that it is empty.

\begin{equation} \label{eq:recall}
	recall = TPR = \frac{\sum True Positive}{\sum Condition Positive}
\end{equation}

\begin{equation} \label{eq:precision}
	precision = PPV = \frac{\sum True Positive}{\sum Predicted Condition Positive}
\end{equation}

\begin{equation} \label{eq:fallout}
	fall\text{-}out = FPR = \frac{\sum False Positive}{\sum Condition Negative}
\end{equation}

\begin{equation} \label{eq:f1score}
	F_1 = 2 \cdot \frac{precision \cdot recall}{precision + recall}
\end{equation}

The F1 score combines the precision (\gls{PPV}) and recall (\gls{TPR}) of a predicted \gls{OGM} to generate a score that is $1$ if the prediction is identical to the ground truth and $0$ if the prediction does not match the ground truth at all (meaning no true positives are predicted). The recall is the probability that a positive grid cell in the ground truth is also predicted as positive. Precision is the probability that a predicted positive grid cell is correctly predicted. \\

The \gls{ROC}-curve is a graphic metric in which the \gls{TPR} is plotted against the \gls{FPR} at various threshold values $\gamma$ for which a grid cell is considered occupied. When predicting \glspl{OGM}, it is desired that the \gls{TPR} is maximized and the \gls{FPR} is minimized. However, this performance depends partly on the threshold $\gamma$. If $\gamma = 0$, all grid cells with a value above $0$ are considered occupied (positive). This means that the \gls{TPR} is equal to $1$, but the \gls{FPR} is equal to $0$. The opposite happens when $\gamma = 1$. Those outcomes are not desired, so the optimal $\gamma$ is often found between $0$ and $1$.  \\

The F1 score and \gls{ROC}-curve evaluate an \gls{OGM} only on a global scale where each grid cell is regarded independently from other grid cells. Therefore, the metric does not distinguish large displacements from small ones or additions and deletions from displacements. It is possible to locally weigh some grid cells more than others if a penalty factor is implemented.



\subsection{What is the best metric to determine the quantitative accuracy of a predicted OGM?}

The evaluation of the criteria which a metric must meet to be suitable for \gls{OGM} prediction is shown in table \ref{tab:metrics}. Based on this chapter's information about the different metrics, the most suitable metrics are the \gls{SSIM} and \gls{IS} metrics. These metrics meet all the criteria. However, to determine whether the \gls{SSIM} is more suitable than the \gls{IS} metric, or whether there is another metric that is more suitable than the metrics from this chapter, more research should be done to investigate \gls{OGM} evaluation metrics. 

\begin{table}
	\centering
	\resizebox{0.5\linewidth}{!}{%
		\begin{tabular}{lllll} 
			\toprule
			& \textbf{\gls{MSE}} & \textbf{\gls{SSIM}} & \textbf{\gls{IS}} & \textbf{F1/\gls{ROC}} \\ 
			\midrule
			\textbf{1.} & YES & YES & YES & YES \\
			\textbf{2. } & NO & YES & YES & NO \\
			\textbf{3. } & NO & YES & YES & NO \\
			\textbf{4. } & YES & YES & YES & YES \\
			\textbf{5.} & NO & YES & YES & NO \\
			\bottomrule
		\end{tabular}
	}
	\caption{This table gives an overview of the metrics that can be used to evaluate \gls{OGM} predictions. For each criterion (numbers 1 to 5), the table shows whether the metric meets it (YES) or not (NO). The criteria are as follows. 
		\\\hspace{\textwidth} 
		\textbf{1.} The metric can evaluate the \gls{OGM} as a whole.
		\textbf{2.} The metric can evaluate the \gls{OGM} on local and on global scale.
		\textbf{3.} The metric negatively weighs small displacements less than big displacements. 
		\textbf{4.} The metric negatively weighs errors close to the \gls{AV} (often the center of the \gls{OGM}) more than errors further from the \gls{AV}.
		\textbf{5.} The metric negatively weighs additions and deletions more than displacements.
	}
	\label{tab:metrics}
\end{table}

% Also a part about loss functions? See Hoerman2020 Loss Functions
