\section{Prediction Methods using Occupancy Grid Maps} \label{sec:ogm_methods}

\subsection{Occupancy Grid Map Prediction Methods} 

%% PredNet progress
\subsubsection{PredNet based OGM predictors}

Itkina \cite{itkina2019dynamic} proposes to utilize Lotter's \cite{lotter2016deep} PredNet, which is an architecture that contains a Convolutional LSTM originally used for video prediction, to predict future occupancy grid map data based on past grid maps. This network is expected to be suitable for \glsfirst{OGM} prediction because an \gls{OGM} is similar to a frame in a video. \\


%% Can be removed or moved to OGM chapter %%
A grid map is an environmental representation that subdivides an area into single cells and estimates the states of each cell based on sensor data. Occupancy grid maps are a form of grid maps were each cell state is represented by its occupancy probability. \cite{nuss2014fusion}. These probabilities can be estimated with methods such as Bayes' inference or the Dempster-Shafer Theory \cite{dempster1968generalization}. For many robotics applications, including AVs, not only the occupancy state of its surroundings is of interest, but also the type of occupancy, such as its class. Nuss \cite{nuss2014fusion} proposed a method to fuse the class labeled information from camera images with the occupancy data from laser data to create occupancy grid maps with labeled objects.    
OGMs are a suitable environment representation for motion prediction because they can generate probabilistic maps without having data of the entire environment or making assumptions of agent behavior \cite{itkina2019dynamic}. \\
%% Can be removed or moved to OGM chapter %%

The convolutional layers are used to correlate the occupied cells in the grid map based on contextual information.
Itkina \cite{itkina2019dynamic} compares the PredNet's performance with a baseline that assumes a static environment (for the short period of time the predictions last), with a FCN network, and with a particle filter predictor. Itkina \cite{itkina2019dynamic} also compares the performance of the network with multiple variations of the input \glspl{OGM}. Using only static \glspl{OGM} is compared with using an additional Dynamic OGM (DOGMa), which includes an additional dimensional layer to the grid map that contains dynamic state information (e.g. velocity). Furthermore, the benefits of the Dempster-Shafer environment representation in the \glspl{OGM} is compared with using a probabilistic alternative. Comparisons are done qualitatively and with the MSE metric that compares each cell of the \glspl{OGM}. 

The research of Itkina \cite{itkina2019dynamic} found that the PredNet outperforms the other investigated methods. Also, the DST-based method performs better than the probability based method, and the difference in performance between the OGM and the DOGMa almost none. For longer predictions, the objects in the OGM become blurry or even disappear from the environment. 

To counter the blurriness and object disappearance for longer term predictions, Lange \cite{lange2020attention} proposes the Attention Augmented ConvLSTM (AAConvLSTM) for Environment Prediction. Lange \cite{lange2020attention} stresses the importance of OGM predictions, because this "approach facilitates the use of occupancy state estimation under uncertainty to update the belief of surroundings" \cite{lange2020attention}. However, the predictions must be reliable, so that is why Lange \cite{lange2020attention} tries to reduce blurriness by implementing an attention mechanism that originated from creating long-term dependencies in language processing into the PredNet architecture. Comparing the results with using the original PredNet architecture, the AAConvLSTM performs better both qualitatively and based on the MSE and Image Similarity (IS) metrics. However, blurriness and disappearance of objects remains.

Toyungyernsub \cite{toyungyernsub2020double} also attempts to counter the blurriness and disappearing objects that resulted from Itkina's \cite{itkina2019dynamic} method. They propose a double-prong ConvLSTM network based on the PredNet architecture which separately predicts the static and dynamic input \glspl{OGM} and fuses the results in a joined prediction of the OGM. The method outperforms the original PredNet both in terms of the IS and MSE metrics as qualitatively. The blurriness and disappearance of objects reduces significantly. For future work, incorporating multi-modality into the predictions is advised because the predicted object orientations were not always correct due to the variety of directions the objects could head for.

The networks of Itkina \cite{itkina2019dynamic}, Lange \cite{lange2020attention}, and Toyungyernsub \cite{toyungyernsub2020double} are trained on the KITTI \cite{geiger2013vision}, Waymo \cite{sun2020scalability} and KITTI \cite{geiger2013vision}, and Waymo \cite{sun2020scalability} datasets respectively.  

%% Hoerman
\subsubsection{Convolutional neural network based OGM predictors}
Hoermann \cite{hoermann2018dynamic}, proposes to predict Dynamic Occupancy Gridmaps (DOGMas) using a convolutional neural network. The network requires one DOGMa of the current timestep and predicts the future $0$ to $3$ seconds. It learns to label the objects, static or dynamic, to provide one static channel output and several dynamic channel outputs that together form an output DOGMa. Only one input DOGMA is used because it is hypothesized that most information necessary for prediction can be found in the dynamic representation and the relation between the cells and not necessarily from the past DOGMas. The paper argues that providing a DOGMa as output is beneficial because it is independent of the sensor setup since the network does not process the raw sensor data directly. Therefore, the network can be trained with varying sensors that provide the input DOGMas. Results show that the proposed method is better than a particle filter approach. The proposed method performs multimodal predictions, can distinguish static from dynamic objects, and provides more accurate predictions than the particle filter. This is because the convolutional layers capture the DOGMa's cell dependencies where particle filters assume independent cells. However, due to uncertainty longer term predictions become vaguer. 

Schreiber \cite{schreiber2019long} builds upon Hoermann's \cite{hoermann2018dynamic} research by expanding the convolutional neural network with an LSTM encoder and decoder, and skip connections with ConvLSTMs. Schreiber \cite{schreiber2019long} states that the prediction of future DOGMas is comparable to a video prediction problem die to the image-like structure of the DOGMas. The LSTMs are expected to exploit spatial and capture temporal correlations, while the skip connection ConvLSTMs are expected to handle missing data due to occlusion. Schreiber's \cite{schreiber2019long} method shows better F1-score results and better qualitative results compared to Hoermann's \cite{hoermann2018dynamic} previous research. The static predictions remain sharp for long term predictions, even for parts of the environment that became partially occluded in the input sequence. Partially occluded dynamic obstacles are predicted accurately as well and there is multimodality in the predictions. Communication signals and road signs are not taken into account. These cues could improve the (multimodal) predictions. Also, the data is only recorded in a stationary scenario, so the performance of the network with egomotion is not researched.   

%% Dequaire and Mohajerin
\subsubsection{Deep tracking based OGM predictors}
Dequaire's \cite{dequaire2018deep} research is based on end-to-end tracking of objects using \glspl{OGM} as environment representations. The research proposes a framework that learns a model of world dynamics in an unsupervised manner. The model can predict the unoccluded state, inluding occupancy, semantics, and dynamic behavior, of the world given a sequence of partially observable input OGMs. This partially observable sequence of the environment provides varying information about the objects that are present. The sequence is used as input for a GRU with dilated convolutional layers to track and classify the partially occluded objects. Dequaire \cite{dequaire2018deep} argues that predicting the future is the same as tracking and classifying the environment given partially occluded \glspl{OGM} of the past and 'future' OGMs that are completely occluded. The network will fill in the future occupancy in the completely occluded OGM based on the information from the past OGMs. Different network configurations' results are compared using the F1 measure and IoU to assess tracking performance and scene semantics quantitatively.  

Mohajerin \cite{mohajerin2019multi} reasons that having OGMs as output provides drivable space information without the need of several stages (detection, classification, tracking, predicting, occupancy grid update) which is required in the classic approach. Therefore, Mohajerin \cite{mohajerin2019multi} builds upon Dequaire's \cite{dequaire2018deep} research and proposes a multi-step prediction of OGMs with a ConvLSTM network architecture. The suggested method learns the difference between consecutive OGMs as a compensation matrix which it then uses to predict the future OGM. This difference learning architecture outperforms Dequaire's \cite{dequaire2018deep} architecture in predicting future OGMs based on the Structural Similarity Index Metric (SSIM). Mohajerin \cite{mohajerin2019multi} did discuss that the provided ground truth data (from the KITTI \cite{geiger2013vision} dataset) is an inadequate target to which the predictions should be compared, since the ground truth only contains the visible borders (to the AV) within its FOV, while the model predicts the whole environment. As a result, the network can accurately predict occupied cells outside the FOV of the ground truth data which is then erroneously considered a false positive because the GT data is not complete.  

%% Representing LiDAR data in BEV maps and predict future BEV maps (OGMs?)
\subsubsection{MotionNet multi-channel BEV map predictor}
Wu's \cite{wu2020motionnet} performs perception and motion prediction with 3D LiDAR data as input. A sequence of LiDAR sweep 3D point clouds synchronized to the current time frame is converted to BEV maps by voxellizing the point cloud and representing it as a 2D pseudo-image where the height dimension corresponds to the image channels. So it is similar to having a multi-channel OGM where each channel is another height. This representation makes convolution possible. The MotionNet makes use of spatio-temporal convolution (STC) blocks that consist of standard 2D convolutions for capturing spatial features, followed by a 3D convolution that captures the temporal features. The output of the network is a BEV grid cell map with occupancy information, object labels, and motion information in 3 channels per timestep. This is similar to a DOGMa with labelling information. Based on the L2 distance metric between the predicted objects and the ground truth, MotionNet outperforms other networks especially because of its ability to distinguish static and dynamic obstacles well. This is because networks learns to classify static objects and links a zero velocity value to them.

% Questions about networks and datasets
% Attention based ConvLSTM research summary etc.
% (Attention can refer to transformer network als alternative)


\subsection{State Prediction Methods} \label{sec:state_methods}

%% Transformer 
\subsubsection{Transformer based predictors}
Li \cite{li2020end} proposes an end-to-end network that performs object detection and trajectory prediction of an AV's environment using it's raw LiDAR and camera data as input in the form of BEV 3D occupancy grid map data. The first part of the network performs multi sensor object detection where LiDAR and camera data are fused to provide spatial and representational features of the surrounding actors. These features are provided as a spatial input sequence of actor data to the Transformer part of the network. The transformer outputs actor interaction features that are processed by an autoregressive recurrent model to predict the future states and features of the actors. These features can be projected onto a grid map for the AV's motion planning purposes. This network is trained and evaluated on the ATG4D \cite{luo2018fast} and NuScenes \cite{caesar2020nuscenes} datasets. The Transformer method outperforms all other methods that were trained on the ATG4D and NuScenes datasets, based on the Detection Average Precision (AP), Average and Final Displacement Error (ADE, FDE), and Trajectory Collision Rate (TCR) metrics. Qualitative results show that collision avoidance is done well compared to the baseline Transformer method. This method did not yet include pedestrians and bicyclists. Because of their more unpredictable behavior, more research should be done to verify that this network also works well on VRU motion prediction.

%% 3D voxel grid maps based on LiDAR data
\subsubsection{LiDAR based OGMs for state predictor input}
Luo \cite{luo2018fast} proposes a real-time end-to-end 3D detection, tracking and motion prediction network which only uses a CNN network and performs a task within 30ms. An AV's LiDAR data is encoded by a 4D tensor of 3D voxel OGMs in space over several time frames. Having 3D input voxel OGMs makes the learning progress easier since the network can use priors about typical object sizes. A CNN performs 3D convolutions on the voxel OGM to detect, track, and predict objects by outputting 3D bounding boxes of the predicted future object locations. For motion forecasting the network performs well and can predict 10 frames with an average L2 distance of 0.33 meter. It has a recall of 92.5\%. It performs qualitatively well for both static and dynamic object forecasting. However, due to the sparsity of the LiDAR 3D points, some objects are not detected properly and therefore not forecasted well. The network is also not tested for pedestrians and longer term predictions.


%% Methods that use a single OGM as contextual information beside past trajectories


\subsection{What method provides the best OGM predictions?}