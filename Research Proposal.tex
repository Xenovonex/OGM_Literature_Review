\section{Research Proposal} \label{sec:res_prop}
Based on the conclusions of the literature study, the following research plan is proposed. First, a title is presented in section \ref{subsec:rp_title}. Then, some background knowledge describing the research problem is described in section \ref{subsec:rp_background}. Subsequently, the research questions are proposed in section \ref{subsec:rp_questions}. After that, the methodology is explained in section \ref{subsec:rp_methodology}. This is followed by a planning in section \ref{subsec:rp_planning} to execute the proposed research. 

\subsection{Title} \label{subsec:rp_title}
The effect of the incorporation of semantic information into the loss function on the performance of an \glsfirst{OGM} prediction network.

\subsection{Background} \label{subsec:rp_background}
Itkina \cite{itkina2019dynamic} observes that the further the \gls{OGM} predictions go into the future, the more objects in the predictions become blurry or disappear entirely. Lange \cite{lange2020attention} reasons that maintaining dynamic objects in the \gls{OGM} predictions leads to higher (worse) \gls{MSE} loss scores due to the multimodal characteristics of traffic prediction. As a result, the dynamic objects are blurred or removed by the network to handle the uncertainty that characterizes multimodality. \\

To solve the problem of blurriness and object disappearance in longer term predictions, \cite{toyungyernsub2020double} and \cite{lange2020attention} tried to develop methods that prevent these disadvantageous effects from happening. Toyungernsub \cite{toyungyernsub2020double} incorporated semantic priors into the \gls{OGM} prediction network that categorizes the \gls{OGM}'s grid cells into 'static' or 'dynamic' labeled grid cells. They adjusted the loss function to weigh the dynamic grid cells more than the static ones. The result was that the predictions' bluriness decreased and fewer objects disappeared. Lange \cite{lange2020attention} and Toyungernsub \cite{toyungyernsub2020double} mention that adding (more) semantic information to the learning process could maintain the dynamic objects for a longer time in the \gls{OGM} predictions. \\

Djuric \cite{djuric2018motionpred} and Li \cite{li2020end} realized that semantic information about the environment is important for predictions. They used object-centered methods that incorporate semantic information of the environment through rasterized \gls{BEV} representations of the road, lane, and occupancy of the \gls{AV}'s surroundings. However, these methods only output state predictions of the detected objects and not of the full \gls{OGM}. Furthermore, the downside of object-centered prediction methods is that occupancy information could get lost or distorted when there are missing or erroneous detections from the raw sensor data (which is more elaborately discussed in chapter \ref{sec:intro} of the literature study). \\

Wu \cite{wu2020motionnet} found a way to incorporate semantics without relying on semantic priors. Moreover, as \cite{lange2020attention} implied, having a loss function that only considers each grid cell individually in its computations can also contribute to the blurring and disappearance of objects. Therefore, Wu's \cite{wu2020motionnet} MotionNet does use semantic information acquired from object detection, but only to compute the loss which trains the network, not as input information. The input to MotionNet is a \gls{BEV} binary occupancy raster, which is basically an \gls{OGM}. The network learns how to make \gls{OGM} predictions with dynamic and semantic classification of each grid cell. To train the network, Wu developed a loss function that combines the loss of the occupancy, semantics, and dynamics in the predictions. The loss also implements the dependencies of the grid cells that belong to the same object. However, the loss consists of six factors that are all based on the L1-loss and where the relations between the grid cells, the objects, and the environment is not considered by the loss. \\

Wu \cite{wu2020motionnet} deemed it important to develop a new loss function to enforce smooth predictions both spatially and temporally. A metric and a loss function share the same purpose, except that a loss function should be differentiable so its derivative can be determined for back-propagation. Therefore, it makes sense that a loss function is tested against the same criteria as a metric to determine its ability to evaluate a specific task. In this case the evaluation task is to compare predicted \glspl{OGM} with ground truth \glspl{OGM}. This evaluation is necessary to train a network to perform \gls{OGM} predictions. In the literature study (Chapter \ref{subsec:lossfunc}), the loss functions that were used in the different \gls{OGM} prediction methods were evaluated against the criteria that were set for the metrics. Chapter \ref{sec:metrics} provides the listing and explanation of the criteria. \\

With regard to these criteria, Wu's \cite{wu2020motionnet} developed loss still would only meet the first criterion and is deemed unsuitable for \gls{OGM} prediction. The literature study discovered in chapter \ref{subsec:lossfunc} that most \gls{OGM} prediction methods using deep learning networks are trained using a loss function that does not consider grid cell dependencies. These loss functions are the L1-loss, L2-loss, \gls{MSE}, and Cross-Entropy loss. These loss functions only meet one of the five criteria (see table \ref{tab:loss_func} in chapter \ref{sec:ogm_methods} of the literature review). However, one method (Mohajerin's \cite{mohajerin2019multi} method in chapter \ref{subsec:deeptrack}), does use a loss function, the \glsfirst{SSIM} (Chapter \ref{subsec:ssim_met}), which meets all five criteria. This loss function considers grid cell dependencies. Furthermore, the \glsfirst{IS} metric, described in chapter \ref{subsec:is_met}, also considers grid cell dependencies. If the \gls{IS} metric were used as a loss function, it would also meet all five criteria. \\

From this background, a problem arises. To reduce or even prevent blurriness and object disappearance in the \gls{OGM} predictions, a method should be developed that incorporates semantic information in the loss computation which also considers dependencies between relations between individual grid cells. This results in the following main research questions and two sub-questions in the next section. 

%TODO:  this paragraph below might be unnecessary
%Dequaire's \cite{dequaire2018deep} network is trained to predict \gls{OGM} and semantic labels for every grid cell in the \gls{OGM}. Dequaire argues that for a deep learning network to learn \gls{OGM} prediction, it learns hidden representations that contain dynamic information about the grid cells which can also be used for classification. Therefore, they first pre-trained a network to predict general \glspl{OGM}, so the network learns those hidden representations of the \gls{OGM} dynamics. Then, they trained a semantic head that could classify each grid cell. However, the classification loss is not backpropagated through the pre-trained \gls{OGM} prediction base of the network. Also, the classification loss is not combined with the occupancy loss, so the effect of adding semantic information during the loss evaluation on the performance of the \gls{OGM} predictions could not be evaluated. 


\subsection{Research Questions} \label{subsec:rp_questions}
The main question for this research proposal results from the background and problem statement and is as follows: \\
\textit{What is the effect of incorporating semantic data into a loss function that considers grid cell dependencies in Deep Learning methods for \glsfirst{OGM} prediction of traffic scenes?} \\

This main question can be answered by investigating the answer to the following two sub-questions:

\begin{enumerate}
	\item \textit{What is the effect on the performance of an \gls{OGM} prediction deep learning network if semantic data is incorporated into the loss function?}
	\item \textit{Does the accuracy of the predictions of an \gls{OGM} prediction deep learning network increase if a loss function is used that considers grid cell dependencies compared to a loss function that evaluates each grid cell independently?}
\end{enumerate}

To answer the questions a methodology is described in the following section.

\subsection{Methodology}  \label{subsec:rp_methodology}
To answer the research questions, a new loss function is required that incorporates semantic data while also considering the dependencies between grid cells. As described in the literature study, to perform \gls{OGM} predictions several components are required. First, a \glspl{OGM} generation method is required. Second, a dataset that contains suitable data for \gls{OGM} generation and prediction should be chosen. Third, four loss functions are required that together either evaluate each grid cell independently, or consider grid cell dependencies, and, either incorporate semantic data or not. Fourth, a network architecture for the \glspl{OGM} prediction should be chosen. Fifth, metrics to evaluate and compare the network's results are required. Finally, experiments can be done to answer the research questions. The choices for the components are discussed in the following subsections. 

\subsubsection{\gls{OGM} generation method} 
The \glspl{OGM} will be generated using the \gls{DST} method. Chapter \ref{sec:ogm} of the literature study shows that this method is the most suitable one to use for \gls{OGM} prediction. Moreover, this method has been used before by the PredNet-based \gls{OGM} prediction methods, which gives a practical reason to use it together with one of those methods.  

\subsubsection{Dataset} 
The Waymo \cite{sun2020scalability} dataset will be used because the literature study shows in chapter \ref{sec:datasets} that it is the best dataset for \gls{OGM} prediction. Furthermore, this dataset contains annotations of detected objects in four categories (vehicles, pedestrians, cyclists, signs) which can be used as semantic data to incorporate into the loss function.  

\subsubsection{Loss Function}
The \gls{MSE} and L1-loss are used as loss functions that evaluate each grid cell independently. These loss functions are already used in most \gls{OGM} prediction methods (see table \ref{tab:method_overview} in chapter \ref{sec:ogm_methods} in the literature study), therefore those loss function have been proven to provide acceptable training results. The \gls{SSIM} is chosen as loss function that considers dependencies between grid cells. Table \ref{tab:metrics} shows that this loss function meet all criteria set for metrics. It is therefore expected it also functions well as loss functions. Furthermore, the \gls{SSIM} is used as loss function by \cite{mohajerin2019multi}, which proves that this loss function provides acceptable training results. Furthermore, two new loss functions must be developed. One loss function will be based on the L1-loss which evaluates each grid cell independently and incorporates semantic data. The other loss will be based on either the \gls{SSIM} or \gls{IS} metric to consider grid cell dependencies, and it will incorporate semantic data.

\subsubsection{Deep Learning Network Architecture} 
Lange's \cite{lange2020attention} \gls{AAConvLSTM} network architecture will be used since it scored highest compared to the other PredNet methods on the Waymo \cite{sun2020scalability} dataset (see table \ref{tab:method_results} in chapter \ref{sec:ogm_methods} in the literature study). Moreover, the code for Lange's \cite{lange2020attention} network is available online. Also, the \gls{DST} method is chosen to generate the \glspl{OGM}, which Lange also uses in their research. This will make it easier to fit the \gls{OGM}'s format to Lange's architecture. Lange's network originally uses a L1-loss for training.

\subsubsection{Metrics} 
The \gls{MSE}, \gls{IS}, and \gls{SSIM} are used as metrics to evaluate the results of the predictions. The \gls{MSE} is chosen because it has been used most often by the other methods (see table \ref{tab:method_overview}). The \gls{IS} and \gls{SSIM} are chosen because they both met all metrics criteria (see table \ref{tab:metrics}). 

\subsubsection{Experiments}
First, the Waymo \cite{sun2020scalability} dataset is used to generate \gls{OGM} sequences using the \gls{DST} method. For every sequence, also the semantic data is stored. Second, the deep learning network is adjusted, if necessary, to fit the \gls{OGM} sequence format as input. Third, the network is trained multiple times on the Waymo \cite{sun2020scalability} dataset. Each time a different loss function is chosen to train the network (independent loss, independent loss + semantics, dependent loss, dependent loss + semantics). Fourth, the results of the network's predictions are compared using the chosen metrics. Based on these comparisons, the research questions can be answered. 

\subsection{Planning} \label{subsec:rp_planning}
This planning gives a monthly overview about when this research proposal will be executed. 


\paragraph{July} Analysis of the Waymo Open - Perception - \cite{sun2020scalability} dataset. Generation of \gls{OGM} sequences from the dataset. Loss function development.
\paragraph{August} Loss function development. Preparation of the deep learning networks for training. Summer Vacation.
\paragraph{September} Preparation the deep learning networks for training.
\paragraph{October} Training the deep learning networks. Review loss function.
\paragraph{November} Analysis of the results and writing thesis. 
\paragraph{December} Writing thesis.
\paragraph{January-February} Graduation (and any unexpected delays).


